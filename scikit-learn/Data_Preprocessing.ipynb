{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化与标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler:\n",
      " [[-0.81977883  0.26445864 -0.26315037  0.95333012 -0.19761598]\n",
      " [ 1.11826642  0.88186034 -1.09584202 -0.70174188  1.64020036]\n",
      " [ 0.08628259 -0.5569763   0.51466318  0.98490138  0.05725663]\n",
      " [-0.12595707 -1.17203613  1.02347643  0.54502596  1.43883752]\n",
      " [ 0.51274303  0.39323329  0.40485894  0.39166915 -1.28173244]] \n",
      "\n",
      "MinMaxScaler:\n",
      " [[0.24310319 0.55735027 0.42174442 0.78325507 0.44150447]\n",
      " [0.79554777 0.74152102 0.18369783 0.28595427 0.95079381]\n",
      " [0.50137823 0.31231646 0.64410269 0.79274131 0.51213389]\n",
      " [0.44087879 0.12884428 0.78956021 0.66057159 0.89499283]\n",
      " [0.62294183 0.59576371 0.61271228 0.61449235 0.14107785]] \n",
      "\n",
      "RobustScaler:\n",
      " [[-0.44324973  0.09548656 -0.17230433  0.59134579 -0.06339512]\n",
      " [ 0.72468106  0.43268204 -0.67178141 -0.34148766  1.10877673]\n",
      " [ 0.10277314 -0.35314218  0.29425499  0.60914002  0.09916437]\n",
      " [-0.02512956 -0.68905864  0.59945867  0.36121696  0.98034614]\n",
      " [ 0.35977244  0.16581716  0.22839063  0.27478183 -0.75485204]] \n",
      "\n",
      "Normalizer:\n",
      " [[0.20590819 0.48018604 0.36027374 0.66978737 0.38550084]\n",
      " [0.52607922 0.50351356 0.12522521 0.20364854 0.64230886]\n",
      " [0.38173322 0.2491681  0.49936699 0.61610723 0.40402908]\n",
      " [0.30094797 0.09744679 0.54767339 0.46224099 0.62154772]\n",
      " [0.4951152  0.48742909 0.49701704 0.50375207 0.12838121]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据归一化与标准化\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X = np.random.rand(100, 5) * 100  # Example dataset\n",
    "\n",
    "# 归一化与标准化\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler(),\n",
    "    \"Normalizer\": Normalizer()\n",
    "}\n",
    "\n",
    "scaled_data = {name: scaler.fit_transform(X) for name, scaler in scalers.items()}\n",
    "\n",
    "# 打印示例结果\n",
    "for name, data in scaled_data.items():\n",
    "    print(f\"{name}:\\n\", data[:5], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项式特征拓展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Features:\n",
      " [[  24.1156465    56.23864104   42.19469928   78.44445351   45.1492575\n",
      "   581.56440616 1356.23118706 1017.55245208 1891.7387108  1088.80353372\n",
      "  3162.78474624 2372.97254676 4411.60946266 2539.13288602 1780.39264745\n",
      "  3309.94012614 1905.05934312 6153.53228644 3541.7088312  2038.45545308]\n",
      " [  77.71678459   74.38319895   18.49930724   30.08465073   94.8871915\n",
      "  6039.89860638 5780.82304997 1437.70667553 2338.08232019 7374.32742201\n",
      "  5532.86028672 1376.03765073 2237.79256079 7058.01284381  342.22436825\n",
      "   556.54519699 1755.34730847  905.08620959 2854.6480152  9003.57911133]\n",
      " [  49.17487722   32.09783665   64.32845928   79.36693899   52.04702892\n",
      "  2418.16854913 1578.40717598 3163.34408635 3902.85948002 2559.40625679\n",
      "  1030.2711174  2064.80437758 2547.50704301 1670.59703237 4138.15067285\n",
      "  5105.552903   3348.10518064 6299.1110054  4130.81336953 2708.8932199 ]\n",
      " [  43.3048972    14.02210291   78.80744374   66.51415166   89.43758737\n",
      "  1875.31412189  607.22572516 3412.74825013 2880.38850011 3873.08552723\n",
      "   196.61937005 1105.0460863   932.66827957 1254.10305421 6210.61318893\n",
      "  5241.81026461 7048.34763489 4424.13237056 5948.86525004 7999.08203441]\n",
      " [  60.96963021   60.023145     61.20382786   62.03319541   15.80915865\n",
      "  3717.29580776 3659.58895458 3731.57475182 3782.14098481  963.87855689\n",
      "  3602.77793566 3673.64623404 3723.4274829   948.91542206 3745.90854443\n",
      "  3796.66901334  967.58102468 3848.11733279  980.6926279   249.92949727]]\n"
     ]
    }
   ],
   "source": [
    "# 多项式特征扩展\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 使用 PolynomialFeatures 生成二次特征\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(\"Polynomial Features:\\n\", X_poly[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据二值化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarized Data:\n",
      " [[0. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 数据二值化\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# 使用 Binarizer 将数据转换为二值\n",
    "binarizer = Binarizer(threshold=50)\n",
    "X_binarized = binarizer.fit_transform(X)\n",
    "\n",
    "print(\"Binarized Data:\\n\", X_binarized[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed Data:\n",
      " [[47.14740744 56.23864104 42.19469928 78.44445351 45.1492575 ]\n",
      " [77.71678459 74.38319895 18.49930724 30.08465073 94.8871915 ]\n",
      " [49.17487722 32.09783665 64.32845928 79.36693899 52.04702892]\n",
      " [43.3048972  14.02210291 78.80744374 66.51415166 89.43758737]\n",
      " [60.96963021 60.023145   61.20382786 62.03319541 15.80915865]]\n"
     ]
    }
   ],
   "source": [
    "# 缺失值处理\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 模拟缺失值\n",
    "X_with_nan = X.copy()\n",
    "X_with_nan[::10, 0] = np.nan  # Add NaN values\n",
    "\n",
    "# 使用 SimpleImputer 填充缺失值\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(X_with_nan)\n",
    "\n",
    "print(\"Imputed Data:\\n\", X_imputed[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (SelectKBest):\n",
      " [[ 2.36539774 -2.40507021  0.63545372  0.25548353 -1.61977676]\n",
      " [ 1.46635883  1.43183788  1.66274087  2.76812878 -0.07690289]\n",
      " [ 0.81812098 -0.40840876  1.70915765  1.03047812  2.56668876]\n",
      " [ 0.17313038 -2.01946068  0.22270649 -0.34009373  2.68224659]\n",
      " [-1.07533851  1.23186307 -1.18685772  3.33030066  1.00210093]]\n",
      "Selected Features (RFE):\n",
      " [[ 2.36539774 -2.40507021  0.63545372  0.25548353 -1.61977676]\n",
      " [ 1.46635883  1.43183788  1.66274087  2.76812878 -0.07690289]\n",
      " [ 0.81812098 -0.40840876  1.70915765  1.03047812  2.56668876]\n",
      " [ 0.17313038 -2.01946068  0.22270649 -0.34009373  2.68224659]\n",
      " [-1.07533851  1.23186307 -1.18685772  3.33030066  1.00210093]]\n"
     ]
    }
   ],
   "source": [
    "# 特征选择\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic classification dataset\n",
    "X, y = make_classification(n_samples=500, n_features=10, n_informative=5, n_redundant=2, random_state=42)\n",
    "\n",
    "# 1. SelectKBest\n",
    "select_k_best = SelectKBest(score_func=f_classif, k=5)\n",
    "X_k_best = select_k_best.fit_transform(X, y)\n",
    "\n",
    "# 2. RFE\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "# 3. SelectFromModel\n",
    "sfm = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=5)\n",
    "X_sfm = sfm.fit_transform(X, y)\n",
    "\n",
    "# 4. Variance Threshold\n",
    "variance_thresh = VarianceThreshold(threshold=0.1)\n",
    "X_variance = variance_thresh.fit_transform(X)\n",
    "\n",
    "print(\"Selected Features (SelectKBest):\\n\", X_k_best[:5])\n",
    "print(\"Selected Features (RFE):\\n\", X_rfe[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码分类变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded:\n",
      " [[0. 0. 1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 1.]]\n",
      "Label Encoded:\n",
      " [2 0 1 2]\n",
      "Ordinal Encoded:\n",
      " [[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# 编码分类变量\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "# 示例分类数据\n",
    "categorical_data = np.array([[\"Red\", \"S\"], [\"Blue\", \"M\"], [\"Green\", \"L\"], [\"Red\", \"XL\"]])\n",
    "ordinal_data = np.array([[\"Low\"], [\"Medium\"], [\"High\"], [\"Low\"]])\n",
    "\n",
    "# 1. OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_onehot = onehot_encoder.fit_transform(categorical_data).toarray()\n",
    "\n",
    "# 2. LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_label = label_encoder.fit_transform(categorical_data[:, 0])\n",
    "\n",
    "# 3. OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[\"Low\", \"Medium\", \"High\"]])\n",
    "encoded_ordinal = ordinal_encoder.fit_transform(ordinal_data)\n",
    "\n",
    "print(\"One-Hot Encoded:\\n\", encoded_onehot)\n",
    "print(\"Label Encoded:\\n\", encoded_label)\n",
    "print(\"Ordinal Encoded:\\n\", encoded_ordinal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理不平衡数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Resampled Class Distribution:\n",
      " [89 89]\n",
      "ADASYN Resampled Class Distribution:\n",
      " [89 90]\n"
     ]
    }
   ],
   "source": [
    "# 处理不平衡数据\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# 模拟不平衡数据\n",
    "X_imbalanced, y_imbalanced = make_classification(\n",
    "    n_classes=2, class_sep=2, weights=[0.9, 0.1], n_informative=3, n_redundant=1, random_state=42\n",
    ")\n",
    "\n",
    "# 1. SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_imbalanced, y_imbalanced)\n",
    "\n",
    "# 2. ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X_imbalanced, y_imbalanced)\n",
    "\n",
    "print(\"SMOTE Resampled Class Distribution:\\n\", np.bincount(y_smote))\n",
    "print(\"ADASYN Resampled Class Distribution:\\n\", np.bincount(y_adasyn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是对每个代码块中工具的原理讲解：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 数据归一化与标准化**\n",
    "\n",
    "#### 工具原理：\n",
    "1. **StandardScaler**:\n",
    "   - 将数据转换为标准正态分布，均值为 0，标准差为 1。\n",
    "   - 公式：\\( z = \\frac{x - \\mu}{\\sigma} \\)，其中 \\( \\mu \\) 是均值，\\( \\sigma \\) 是标准差。\n",
    "   - **适用场景**: 数据分布有较大差异时，尤其是线性模型和 SVM。\n",
    "\n",
    "2. **MinMaxScaler**:\n",
    "   - 将数据缩放到指定范围（默认 [0,1]）。\n",
    "   - 公式：\\( z = \\frac{x - x_{min}}{x_{max} - x_{min}} \\)。\n",
    "   - **适用场景**: 数据范围固定，适用于距离度量敏感的模型（如 KNN）。\n",
    "\n",
    "3. **RobustScaler**:\n",
    "   - 基于中位数和四分位数缩放，减少异常值的影响。\n",
    "   - **适用场景**: 数据中存在异常值时。\n",
    "\n",
    "4. **Normalizer**:\n",
    "   - 将每个样本（行）归一化为单位向量。\n",
    "   - **适用场景**: 数据稀疏或分布不均时，尤其是文本和推荐系统。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 多项式特征扩展**\n",
    "\n",
    "#### 工具原理：\n",
    "- **PolynomialFeatures**:\n",
    "  - 为数据生成多项式组合特征。\n",
    "  - 公式：假设有 2 个特征 \\( x_1, x_2 \\)，二次特征扩展为 \\( [x_1, x_2, x_1^2, x_2^2, x_1 \\cdot x_2] \\)。\n",
    "  - **适用场景**: 通过非线性特征扩展提高模型性能，适用于线性模型和树模型。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 数据二值化**\n",
    "\n",
    "#### 工具原理：\n",
    "- **Binarizer**:\n",
    "  - 将数据按阈值转化为二进制值（0 或 1）。\n",
    "  - **适用场景**: 将连续特征离散化，适用于逻辑回归等。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 缺失值处理**\n",
    "\n",
    "#### 工具原理：\n",
    "- **SimpleImputer**:\n",
    "  - 使用均值、中位数或最频繁值填充缺失值。\n",
    "  - 公式：\\( x_{missing} = \\text{填充值} \\)。\n",
    "  - **适用场景**: 数据存在缺失值时，尤其是数值型数据。\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 特征选择**\n",
    "\n",
    "#### 工具原理：\n",
    "1. **SelectKBest**:\n",
    "   - 基于统计量选择前 \\( k \\) 个重要特征。\n",
    "   - 公式：计算每个特征与目标变量的相关性评分（如 ANOVA F 值）。\n",
    "   - **适用场景**: 高维数据。\n",
    "\n",
    "2. **RFE (Recursive Feature Elimination)**:\n",
    "   - 递归删除不重要特征，直到剩余目标数量的特征。\n",
    "   - 通过反复训练模型，移除特征权重低的特征。\n",
    "   - **适用场景**: 线性模型、决策树。\n",
    "\n",
    "3. **SelectFromModel**:\n",
    "   - 基于模型特征的重要性得分（如随机森林的 Gini 指数）。\n",
    "   - **适用场景**: 使用树模型时。\n",
    "\n",
    "4. **VarianceThreshold**:\n",
    "   - 删除低方差特征。\n",
    "   - **适用场景**: 特征对分类任务贡献较低。\n",
    "\n",
    "---\n",
    "\n",
    "### **6. 编码分类变量**\n",
    "\n",
    "#### 工具原理：\n",
    "1. **OneHotEncoder**:\n",
    "   - 将分类变量编码为独热编码。\n",
    "   - 适合无序分类变量。\n",
    "   - 公式：假设有 3 类：`A, B, C`，编码为 `[1, 0, 0]`，`[0, 1, 0]`，`[0, 0, 1]`。\n",
    "\n",
    "2. **LabelEncoder**:\n",
    "   - 将分类标签转换为整数编码。\n",
    "   - **适用场景**: 有序或无序分类变量。\n",
    "\n",
    "3. **OrdinalEncoder**:\n",
    "   - 类似于 LabelEncoder，但支持多列编码。\n",
    "   - **适用场景**: 序数型特征，如 `Low, Medium, High`。\n",
    "\n",
    "---\n",
    "\n",
    "### **7. 处理不平衡数据**\n",
    "\n",
    "#### 工具原理：\n",
    "1. **SMOTE** (Synthetic Minority Oversampling Technique):\n",
    "   - 通过插值生成少数类样本。\n",
    "   - 在少数类样本之间随机选择一个样本，然后生成新样本。\n",
    "   - **适用场景**: 分类任务中类别不平衡。\n",
    "\n",
    "2. **ADASYN** (Adaptive Synthetic Sampling):\n",
    "   - 自适应生成少数类样本，聚焦于边界附近的样本。\n",
    "   - **适用场景**: 与 SMOTE 类似，但适合样本分布复杂的数据。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

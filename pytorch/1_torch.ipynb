{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "is_tensor\n",
    "\n",
    "is_storage\n",
    "\n",
    "is_complex\n",
    "\n",
    "is_conj\n",
    "\n",
    "is_floating_point\n",
    "\n",
    "is_nonzero\n",
    "\n",
    "set_default_dtype\n",
    "\n",
    "get_default_dtype\n",
    "\n",
    "set_default_device\n",
    "\n",
    "get_default_device\n",
    "\n",
    "numel\n",
    "\n",
    "set_printoptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 1. is_tensor()判断一个对象是否是张量，需要专门用torch的tensor来创建张量，否则不是张量\n",
    "# 创建张量\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(torch.is_tensor(x))  # 输出: True\n",
    "\n",
    "# 非张量\n",
    "y = [1, 2, 3]\n",
    "print(torch.is_tensor(y))  # 输出: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epiph\\AppData\\Local\\Temp\\ipykernel_9364\\3205632684.py:4: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = x.storage()\n"
     ]
    }
   ],
   "source": [
    "# is_storage()判断一个对象是否是存储对象，也是需要用torch的tensor来创建张量，然后通过storage()方法来获取存储对象\n",
    "# 创建张量的存储对象\n",
    "x = torch.tensor([1, 2, 3])\n",
    "storage = x.storage()\n",
    "print(torch.is_storage(storage))  # 输出: True\n",
    "\n",
    "# 非存储对象\n",
    "print(torch.is_storage(x))  # 输出: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#  is_complex()判断一个张量是否是复数类型。\n",
    "# 复数张量\n",
    "x = torch.tensor([1 + 1j, 2 + 2j])\n",
    "print(torch.is_complex(x))  # 输出: True\n",
    "\n",
    "# 非复数张量\n",
    "y = torch.tensor([1, 2, 3])\n",
    "print(torch.is_complex(y))  # 输出: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# is_conj()判断张量是否为共轭张量。\n",
    "\n",
    "x = torch.tensor([1 + 1j, 2 + 2j])\n",
    "y = x.conj()\n",
    "print(y.is_conj())  # 输出: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# is_floating_point()判断张量是否为浮点数类型。\n",
    "x = torch.tensor([1.5, 2.5, 3.5])\n",
    "print(torch.is_floating_point(x))  # 输出: True\n",
    "\n",
    "y = torch.tensor([1, 2, 3])\n",
    "print(torch.is_floating_point(y))  # 输出: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# is_nonzero()判断张量是否为非零标量。\n",
    "# 标量非零值\n",
    "x = torch.tensor(3)\n",
    "print(x.is_nonzero())  # 输出: True\n",
    "\n",
    "# 标量零值\n",
    "y = torch.tensor(0)\n",
    "print(y.is_nonzero())  # 输出: False\n",
    "\n",
    "# 非标量的张量会报错\n",
    "z = torch.tensor([1, 2, 3])\n",
    "# z.is_nonzero()  # 会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# is_nonzero()判断张量是否为非零标量，注意这里只能判断\n",
    "# 标量非零值\n",
    "x = torch.tensor(3)\n",
    "print(x.is_nonzero())  # 输出: True\n",
    "\n",
    "# 标量零值\n",
    "y = torch.tensor(0)\n",
    "print(y.is_nonzero())  # 输出: False\n",
    "\n",
    "# 非标量的张量会报错\n",
    "z = torch.tensor([1, 2, 3])\n",
    "# z.is_nonzero()  # 会报错\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# set_default_dtype()设置默认的浮点数数据类型。\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "x = torch.tensor([1.5, 2.5])\n",
    "print(x.dtype)  # 输出: torch.float64\n",
    "\n",
    "# get_default_dtype()获取当前默认的浮点数数据类型。\n",
    "dtype = torch.get_default_dtype()\n",
    "print(dtype)  # 输出当前默认的浮点数类型，例如 torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# set_default_device()设置默认设备（如CPU或GPU）。\n",
    "torch.set_default_device('cpu')  # 设置默认设备为CPU\n",
    "x = torch.tensor([1.5, 2.5])\n",
    "print(x.device)  # 输出: cuda:0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# numel()返回张量中的元素总数。\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(torch.numel(x))  # 输出: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.235, 2.346])\n"
     ]
    }
   ],
   "source": [
    "# set_printoptions()\n",
    "torch.set_printoptions(precision=3)  # 设置打印精度为3位小数\n",
    "x = torch.tensor([1.234567, 2.345678])\n",
    "print(x)  # 输出: tensor([1.235, 2.346])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建操作（Creation Ops）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# tensor()创建一个张量，类似于 numpy 中的 array()，但没有自动求导历史。\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(x)\n",
    "# 输出:\n",
    "# tensor([[1, 2],\n",
    "#         [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 2]]),\n",
      "       values=tensor([1, 2, 3]),\n",
      "       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n",
      "tensor(crow_indices=tensor([0, 2, 3]),\n",
      "       col_indices=tensor([0, 2, 1]),\n",
      "       values=tensor([1, 2, 3]), size=(2, 3), nnz=3, layout=torch.sparse_csr)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epiph\\.conda\\envs\\torchproject\\Lib\\site-packages\\torch\\utils\\_device.py:77: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ..\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# sparse_coo_tensor()创建一个稀疏 COO 格式的张量，提供值和对应的索引。\n",
    "values = torch.tensor([1, 2, 3])\n",
    "indices = torch.tensor([[0, 1, 1], [2, 0, 2]])\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices, values, [2, 3])\n",
    "print(sparse_tensor)\n",
    "# 输出:\n",
    "# tensor(indices=tensor([[0, 1, 1],\n",
    "#                        [2, 0, 2]]),\n",
    "#        values=tensor([1, 2, 3]),\n",
    "#        size=(2, 3), nnz=3, layout=torch.sparse_coo)\n",
    "\n",
    "\n",
    "# sparse_csr_tensor()创建一个稀疏 CSR 格式的张量。\n",
    "crow_indices = torch.tensor([0, 2, 3])\n",
    "col_indices = torch.tensor([0, 2, 1])\n",
    "values = torch.tensor([1, 2, 3])\n",
    "sparse_csr = torch.sparse_csr_tensor(crow_indices, col_indices, values)\n",
    "print(sparse_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# asarray()将对象转换为张量。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "arr = np.array([1, 2, 3])\n",
    "# arr = [1, 2, 3]二者都可以转换\n",
    "# arr2 = pd.Series([1, 2, 3]) pandas 的Series对象也可以转换\n",
    "x = torch.asarray(arr)\n",
    "print(x)  # 输出: tensor([1, 2, 3], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# as_tensor()与 tensor() 类似，但它共享数据并保留自动求导历史。\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3])\n",
    "x = torch.as_tensor(arr)\n",
    "print(x)  # 输出: tensor([1, 2, 3], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# from_numpy()从 numpy 数组创建张量。\n",
    "arr = np.array([1.0, 2.0, 3.0])\n",
    "x = torch.from_numpy(arr)\n",
    "print(x)  # 输出: tensor([1., 2., 3.], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# zeros()创建一个全为 0 的张量。\n",
    "x = torch.zeros(2, 3)\n",
    "print(x)\n",
    "# 输出:\n",
    "# tensor([[0., 0., 0.],\n",
    "#         [0., 0., 0.]])\n",
    "\n",
    "\n",
    "# ones()创建一个全为 1 的张量。\n",
    "x = torch.ones(2, 3)\n",
    "print(x)\n",
    "# 输出:\n",
    "# tensor([[1., 1., 1.],\n",
    "#         [1., 1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([0.000, 0.250, 0.500, 0.750, 1.000])\n"
     ]
    }
   ],
   "source": [
    "# arange()创建一个从 start 到 end（不包含 end）的张量，间隔为 step。\n",
    "x = torch.arange(0, 10, 2) # 从0开始，步长为2，到10结束（不包含10）\n",
    "print(x)  #    输出: tensor([0, 2, 4, 6, 8])\n",
    "\n",
    "\n",
    "# linspace()创建一个包含 steps 个均匀间隔的数值的张量，范围从 start 到 end。\n",
    "x = torch.linspace(0, 1, steps=5) # steps代表步长，也就是张量元素数量\n",
    "print(x)  # 输出: tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# eye()创建一个对角线为 1 的单位矩阵。\n",
    "x = torch.eye(3)\n",
    "print(x)\n",
    "# 输出:\n",
    "# tensor([[1., 0., 0.],\n",
    "#         [0., 1., 0.],\n",
    "#         [0., 0., 1.]])\n",
    "\n",
    "# empty()创建一个未初始化的数据张量。\n",
    "x = torch.empty(2, 3)\n",
    "print(x)  # 输出: 未初始化的随机数据，无限接近0，小概率不为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 7, 7],\n",
      "        [7, 7, 7]])\n"
     ]
    }
   ],
   "source": [
    "# full()创建一个大小为 size 的张量，并用 fill_value 填充。\n",
    "x = torch.full((2, 3), 7) # (2, 3)代表张量大小，7代表填充值，这里只能填充相同的数\n",
    "print(x)\n",
    "# 输出:\n",
    "# tensor([[7, 7, 7],\n",
    "#         [7, 7, 7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.+3.j, 2.+4.j])\n"
     ]
    }
   ],
   "source": [
    "# complex()构造一个复数张量。\n",
    "real = torch.tensor([1.0, 2.0])\n",
    "imag = torch.tensor([3.0, 4.0])\n",
    "z = torch.complex(real, imag)\n",
    "print(z)\n",
    "# 输出: tensor([1.+3.j, 2.+4.j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.673e-06+1.000e+00j, -2.000e+00-1.469e-05j])\n"
     ]
    }
   ],
   "source": [
    "# polar()使用极坐标创建复数张量。\n",
    "abs_values = torch.tensor([1.0, 2.0])\n",
    "angles = torch.tensor([3.1416/2, 3.1416])\n",
    "z = torch.polar(abs_values, angles)\n",
    "print(z)\n",
    "# 输出: tensor([6.1232e-17+1.0000j, -2.0000+0.0000j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing, Slicing, Joining, Mutating Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.-1.j, 3.-3.j],\n",
      "        [2.-2.j, 4.-4.j]])\n"
     ]
    }
   ],
   "source": [
    "# adjoint()返回一个张量的共轭转置（只作用于最后两个维度）。\n",
    "x = torch.tensor([[1 + 1j, 2 + 2j], [3 + 3j, 4 + 4j]])\n",
    "adjoint_x = x.adjoint()\n",
    "print(adjoint_x)\n",
    "# 输出: \n",
    "# tensor([[1.-1.j, 3.-3.j],\n",
    "#         [2.-2.j, 4.-4.j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [1, 0],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# argwhere()返回张量中所有非零元素的索引。\n",
    "x = torch.tensor([[1, 0], [2, 3]])\n",
    "indices = torch.argwhere(x)\n",
    "print(indices)\n",
    "# 输出: \n",
    "# tensor([[0, 0],\n",
    "#         [1, 0],\n",
    "#         [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# cat()将张量沿指定维度连接。\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "y = torch.tensor([[5, 6], [7, 8]])\n",
    "z = torch.cat((x, y), dim=0) # dim=0代表按行连接\n",
    "print(z)\n",
    "# 输出: \n",
    "# tensor([[1, 2],\n",
    "#         [3, 4],\n",
    "#         [5, 6],\n",
    "#         [7, 8]])\n",
    "z = torch.cat((x, y), dim=1) # dim=1代表按列连接\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "tensor([3, 4])\n",
      "tensor([5, 6])\n",
      "(tensor([1, 2]), tensor([3, 4]), tensor([5, 6]))\n"
     ]
    }
   ],
   "source": [
    "# chunk()将张量分割为指定数量的块。\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "chunks = torch.chunk(x, 3)\n",
    "for c in chunks:\n",
    "    print(c)\n",
    "# 输出: \n",
    "# tensor([1, 2])\n",
    "# tensor([3, 4])\n",
    "# tensor([5, 6])\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 2],\n",
      "        [4, 3, 4]])\n",
      "tensor([[3, 4],\n",
      "        [3, 2]])\n"
     ]
    }
   ],
   "source": [
    "# gather()根据索引从张量中收集值。\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "index = torch.tensor([[0, 0, 1], [1, 0, 1]])\n",
    "output = torch.gather(x, dim=1, index=index) # dim=1代表按列收集，index代表索引，前面的0，0代表选取两次第一行第一列，后面的1，0代表选取第二行第二列和第二行第一列\n",
    "# dim=1表示按列收集，所以第一个[]表示收集第一列，第二个[]表示收集第二列，中间的数字代表具体在这一列取的值\n",
    "\n",
    "print(output)\n",
    "# 输出: \n",
    "# tensor([[1, 1, 2],\n",
    "#         [4, 3, 4]])\n",
    "\n",
    "index = torch.tensor([[1, 1], [1, 0]])\n",
    "output = torch.gather(x, dim=0, index=index) # dim=0\n",
    "# dim=0表示按行收集，所以第两个[]的第一个数表示收集第一列，两个[]的第二个数表示手机第二列\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# index_select()根据索引选择张量的某一维度。\n",
    "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "indices = torch.tensor([0, 2])\n",
    "output = torch.index_select(x, dim=0, index=indices) # dim为0代表找最外层的维度的元素，通常来说dim数值越小，代表越外层的维度，中间取出来的元素也会更大\n",
    "print(output)\n",
    "# 输出: \n",
    "# tensor([[1, 2],\n",
    "#         [5, 6]])\n",
    "\n",
    "indices = torch.tensor([0, 1])\n",
    "output = torch.index_select(x, dim=1, index=indices) # dim为1代表第二外层的元素，在这里就是内层的，只有两个元素，index不可以超过1\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# masked_select()根据布尔掩码选择张量中的元素。\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "mask = torch.tensor([0, 1, 0, 1], dtype=torch.bool)\n",
    "output = torch.masked_select(x, mask) # 模型mask操作步骤非常常用，一般来说首先创建含有随机数量0的张量，对应过程张量，然后相乘，达到mask效果，维度不会改变，但是长度会改变\n",
    "print(output)\n",
    "# 输出: tensor([2, 4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# narrow()返回一个张量的指定维度上的窄视图。\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "narrow_x = torch.narrow(x, dim=1, start=1, length=2) # 从dim=1开始，取2个元素，且起始位置是第1号位\n",
    "print(narrow_x)\n",
    "# 输出: \n",
    "# tensor([[2, 3],\n",
    "#         [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# permute()返回一个张量的维度重新排列后的视图。\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "perm_x = x.permute(1, 0) # 将第一维和第二维交换，相当于转置\n",
    "print(perm_x)\n",
    "# 输出: \n",
    "# tensor([[1, 4],\n",
    "#         [2, 5],\n",
    "#         [3, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# reshape()返回具有相同数据但形状不同的张量。\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "reshaped_x = x.reshape(3, 2) # 从左到右，先看最外层，再往里看，最外层是3，内层是2，所以先取3个元素，每个元素有2个元素，所以取3*2个元素\n",
    "print(reshaped_x)\n",
    "# 输出: \n",
    "# tensor([[1, 2],\n",
    "#         [3, 4],\n",
    "#         [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1, 1],\n",
      "        [2, 1],\n",
      "        [3, 1]])\n"
     ]
    }
   ],
   "source": [
    "# squeeze()移除大小为1的指定维度。\n",
    "x = torch.tensor([[[1], [2], [3]]]) # 将外层的维度去掉，只保留最内层维度\n",
    "squeezed_x = torch.squeeze(x)\n",
    "print(squeezed_x)\n",
    "# 输出: tensor([1, 2, 3])\n",
    "\n",
    "x = torch.tensor([[[1,1], [2,1], [3,1]]]) # 将外层的维度去掉，只保留最内层维度，但是最内层维度有两个，所以保留两个维度，不能破坏数据本身的结构\n",
    "squeezed_x = torch.squeeze(x)\n",
    "print(squeezed_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n"
     ]
    }
   ],
   "source": [
    "# stack()沿着新维度连接张量。\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "stacked = torch.stack((x, y), dim=0)\n",
    "print(stacked)\n",
    "# 输出: \n",
    "# tensor([[1, 2],\n",
    "#         [3, 4]])\n",
    "stacked = torch.stack((x, y), dim=1)\n",
    "print(stacked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3],\n",
      "        [2, 4]])\n"
     ]
    }
   ],
   "source": [
    "# transpose()返回转置后的张量。\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "transposed_x = torch.transpose(x, 0, 1)# 将第一维和第二维交换，相当于转置，0，1和1，0效果一样\n",
    "print(transposed_x)\n",
    "# 输出: \n",
    "# tensor([[1, 3],\n",
    "#         [2, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze()在指定位置插入一个大小为1的新维度。\n",
    "x = torch.tensor([1, 2, 3])\n",
    "unsqueezed_x = torch.unsqueeze(x, dim=0)\n",
    "print(unsqueezed_x)\n",
    "# 输出: \n",
    "# tensor([[1, 2, 3]])\n",
    "\n",
    "unsqueezed_x = torch.unsqueeze(x, dim=1)\n",
    "print(unsqueezed_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# where()根据条件选择元素。\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "condition = torch.tensor([True, False, True]) # 这里的condition待会作用于x和y，如果为True，则取x的值，否则取y的值\n",
    "output = torch.where(condition, x, y)\n",
    "print(output)\n",
    "# 输出: tensor([1, 5, 3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator\n",
    "在 PyTorch 中，Generator 用于创建和管理随机数生成器对象，该对象可以控制随机数生成算法的状态。通过使用 Generator，你可以更精确地控制伪随机数生成器的种子和行为，这在需要可重复实验结果时非常有用。\n",
    "\n",
    "主要用途\n",
    "通过指定种子来生成确定性的伪随机数序列。\n",
    "可以在不同的设备（如 CPU 或 GPU）上管理随机数生成状态。\n",
    "创建 Generator 对象的实例\n",
    "你可以通过 torch.Generator() 来创建生成器，并使用 generator.manual_seed() 方法为其设定种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.058, 0.063, 0.124])\n",
      "tensor([0.053, 0.526, 0.477])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个 Generator 对象\n",
    "gen = torch.Generator()\n",
    "\n",
    "# 手动设置种子，确保生成相同的随机数序列\n",
    "gen.manual_seed(42)\n",
    "\n",
    "# 使用该生成器生成随机数\n",
    "random_tensor = torch.rand(3, generator=gen)\n",
    "print(random_tensor)\n",
    "\n",
    "# 再次生成随机数，因种子相同，所以结果相同\n",
    "random_tensor2 = torch.rand(3, generator=gen)\n",
    "print(random_tensor2)\n",
    "\n",
    "# 生成器的状态保存在 `gen` 中，可以随时修改或查询\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.058, 0.063, 0.124])\n"
     ]
    }
   ],
   "source": [
    "# seed()\n",
    "# seed() 方法为所有设备（如 CPU 和 GPU）的随机数生成器设置一个非确定性（随机的）种子。\n",
    "# 这意味着每次调用时会生成一个不同的随机种子，适用于希望每次执行代码时生成不同随机数的情况。\n",
    "torch.seed()  # 设置一个随机种子\n",
    "\n",
    "# manual_seed(seed) 用于在所有设备上手动设置一个确定的随机数种子。\n",
    "\n",
    "torch.manual_seed(42)  # 设置一个固定的随机数种子\n",
    "\n",
    "# 生成随机张量\n",
    "random_tensor = torch.rand(3)\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42,  0,  0,  ...,  0,  0,  0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# get_rng_state() 返回当前设备上随机数生成器的状态，通常以 torch.ByteTensor 的形式表示。\n",
    "# 你可以通过保存这个状态来确保随后的随机数生成能够恢复到当前状态。\n",
    "rng_state = torch.get_rng_state()\n",
    "print(rng_state)  # 打印当前随机数生成器状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.053, 0.526, 0.477])\n",
      "tensor([0.053, 0.526, 0.477])\n"
     ]
    }
   ],
   "source": [
    "# set_rng_state(state) 用于设置设备上随机数生成器的状态。\n",
    "# 你可以通过先使用 get_rng_state() 保存状态，再使用 set_rng_state() 恢复到之前的状态，确保随机数生成的顺序不被打乱。\n",
    "# 保存当前随机数生成器状态\n",
    "rng_state = torch.get_rng_state()\n",
    "\n",
    "# 生成一个随机张量\n",
    "random_tensor = torch.rand(3)\n",
    "print(random_tensor)\n",
    "\n",
    "# 恢复随机数生成器状态\n",
    "torch.set_rng_state(rng_state)\n",
    "\n",
    "# 再次生成随机张量，应该与之前相同\n",
    "random_tensor = torch.rand(3)\n",
    "print(random_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

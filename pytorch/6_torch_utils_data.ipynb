{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Also known as dataloader part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Dataset_A(data_path, model):\n",
    "    \"\"\"\n",
    "    Load Dataset A. Since the dataset does not provide code, we follow the original literature to preprocess the data.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): dataset path.\n",
    "        model (str): 'fNIRS-T' or 'fNIRS-PreT'. fNIRS-T uses preprocessed data, and fNIRS-PreT uses raw data.\n",
    "\n",
    "    Returns:\n",
    "        feature : fNIRS signal data.\n",
    "        label : fNIRS labels.\n",
    "    \"\"\"\n",
    "    assert model in ['fNIRS-T', 'fNIRS-PreT', 'fNIRS_TTT', \"fNIRS_TTT_bs16\"], ''' The model parameter is 'fNIRS-T' or 'fNIRS-PreT' or 'fNIRS_TTT' '''\n",
    "    feature = []\n",
    "    label = []\n",
    "    for sub in range(1, 3):\n",
    "        if sub >= 4:\n",
    "            trial_num = 4\n",
    "        else:\n",
    "            trial_num = 3\n",
    "        for i in range(1, trial_num + 1):\n",
    "            times = i\n",
    "            path = data_path + '/S' + str(sub) + '/s' + str(sub) + str(times) + '_hb.xls'\n",
    "            hb = pd.read_excel(path, header=None)\n",
    "            path = data_path + '/S' + str(sub) + '/s' + str(sub) + str(times) + '_trial.xls'\n",
    "            trial = pd.read_excel(path, header=None)\n",
    "            path = data_path + '/S' + str(sub) + '/s' + str(sub) + str(times) + '_y.xls'\n",
    "            y = pd.read_excel(path, header=None)\n",
    "\n",
    "            hb = np.array(hb)\n",
    "            trial = np.array(trial)\n",
    "            # 1:MA, 2:REST\n",
    "            y = np.array(y)\n",
    "\n",
    "            HbO = hb[:, 0:52]\n",
    "            HbR = hb[:, 52:104]\n",
    "\n",
    "            # fNIRS-T uses preprocessed data\n",
    "            if model == 'fNIRS-T' or model == 'fNIRS_TTT' or model == 'fNIRS_TTT_bs16':\n",
    "                b, a = signal.butter(4, 0.018, 'lowpass')\n",
    "                HbO = signal.filtfilt(b, a, HbO, axis=0)\n",
    "                HbR = signal.filtfilt(b, a, HbR, axis=0)\n",
    "                b, a = signal.butter(3, 0.002, 'highpass')\n",
    "                HbO = signal.filtfilt(b, a, HbO, axis=0)\n",
    "                HbR = signal.filtfilt(b, a, HbR, axis=0)\n",
    "\n",
    "            HbO = HbO.transpose((1, 0))\n",
    "            HbR = HbR.transpose((1, 0))\n",
    "\n",
    "            MA_tr = []\n",
    "            REST_tr = []\n",
    "            for i in range(y.shape[0]):\n",
    "                if y[i, 0] == 1:\n",
    "                    MA_tr.append(trial[i, 0])\n",
    "                else:\n",
    "                    REST_tr.append(trial[i, 0])\n",
    "\n",
    "            HbO_MA = []\n",
    "            HbO_BL = []\n",
    "            HbR_MA = []\n",
    "            HbR_BL = []\n",
    "\n",
    "            for i in range(int(len(MA_tr))):\n",
    "                tr = MA_tr[i]\n",
    "                HbO_MA.append(HbO[:, tr: tr + 140])\n",
    "                HbR_MA.append(HbR[:, tr: tr + 140])\n",
    "                HbO_BL.append(HbO[:, tr + 160: tr + 300])\n",
    "                HbR_BL.append(HbR[:, tr + 160: tr + 300])\n",
    "\n",
    "            # fNIRS channels = 52, sampling points = 140\n",
    "            HbO_MA = np.array(HbO_MA).reshape((-1, 1, 52, 140))\n",
    "            HbO_BL = np.array(HbO_BL).reshape((-1, 1, 52, 140))\n",
    "            HbR_MA = np.array(HbR_MA).reshape((-1, 1, 52, 140))\n",
    "            HbR_BL = np.array(HbR_BL).reshape((-1, 1, 52, 140))\n",
    "\n",
    "            HbO_MA = np.concatenate((HbO_MA, HbR_MA), axis=1)\n",
    "            HbO_BL = np.concatenate((HbO_BL, HbR_BL), axis=1)\n",
    "\n",
    "            for i in range(HbO_MA.shape[0]):\n",
    "                feature.append(HbO_MA[i, :, :, :])\n",
    "                feature.append(HbO_BL[i, :, :, :])\n",
    "\n",
    "                label.append(0)\n",
    "                label.append(1)\n",
    "\n",
    "        print(str(sub) + '  OK')\n",
    "\n",
    "    feature = np.array(feature)\n",
    "    label = np.array(label)\n",
    "    print('feature', feature.shape)\n",
    "    print('label', label.shape)\n",
    "\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Load data for training\n",
    "\n",
    "    Args:\n",
    "        feature: input data.\n",
    "        label: class for input data.\n",
    "        transform: Z-score normalization is used to accelerate convergence (default:True).\n",
    "    \"\"\"\n",
    "    def __init__(self, feature, label, transform=True):\n",
    "        self.feature = feature\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.feature = torch.tensor(self.feature, dtype=torch.float)\n",
    "        self.label = torch.tensor(self.label, dtype=torch.float)\n",
    "        print(self.feature.shape)\n",
    "        print(self.label.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # z-score normalization\n",
    "        if self.transform:\n",
    "            mean, std = self.feature[item].mean(), self.feature[item].std()\n",
    "            self.feature[item] = (self.feature[item] - mean) / std\n",
    "\n",
    "        return self.feature[item], self.label[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  OK\n",
      "2  OK\n",
      "feature (72, 2, 52, 140)\n",
      "label (72,)\n"
     ]
    }
   ],
   "source": [
    "feature, label = Load_Dataset_A(\"data_example\", model='fNIRS-T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 140\n"
     ]
    }
   ],
   "source": [
    "_, _, channels, sampling_points = feature.shape\n",
    "print(channels, sampling_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Dataset_A(sub, feature, label, channels):\n",
    "    \"\"\"\n",
    "    LOSO-CV for Dataset A\n",
    "\n",
    "    Args:\n",
    "        sub: leave one subject out.\n",
    "        feature: input fNIRS signals.\n",
    "        label: input fNIRS labels.\n",
    "        channels: fNIRS channels.\n",
    "\n",
    "    Returns:\n",
    "        X_train: training set.\n",
    "        y_train: labels for training set.\n",
    "        X_test: test set.\n",
    "        y_test: labels for test set.\n",
    "    \"\"\"\n",
    "    if sub == 1:\n",
    "        X_test = feature[: 36]\n",
    "        y_test = label[: 36]\n",
    "        X_train = feature[36:]\n",
    "        y_train = label[36:]\n",
    "    elif sub == 2:\n",
    "        X_test = feature[300:]\n",
    "        y_test = label[300:]\n",
    "        X_train = feature[:300]\n",
    "        y_train = label[: 300]\n",
    "    else:\n",
    "        start, end = 0, 0\n",
    "        if sub in [2, 3]:\n",
    "            start = 36 * (sub - 1)\n",
    "            end = 36 * sub\n",
    "        elif sub in [4, 5, 6, 7]:\n",
    "            start = 108 + 48 * (sub - 4)\n",
    "            end = 108 + 48 * (sub - 3)\n",
    "\n",
    "        X_test = feature[start: end]\n",
    "        y_test = label[start: end]\n",
    "        feature_set_1 = feature[: start]\n",
    "        label_set_1 = label[:start]\n",
    "        feature_set_2 = feature[end:]\n",
    "        label_set_2 = label[end:]\n",
    "        X_train = np.append(feature_set_1, feature_set_2, axis=0)\n",
    "        y_train = np.append(label_set_1, label_set_2, axis=0)\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], 2, channels, -1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 2, channels, -1))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in range(1, 3):\n",
    "    X_train, y_train, X_test, y_test = Split_Dataset_A(sub, feature, label, channels)\n",
    "    train_set = Dataset(X_train, y_train, transform=True)\n",
    "    test_set = Dataset(X_test, y_test, transform=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
